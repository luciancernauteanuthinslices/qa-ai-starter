# QA AI Starter

A comprehensive test automation framework that combines traditional QA practices with AI-powered test generation. This starter kit provides everything needed to build a modern quality assurance pipeline from story creation to automated testing and reporting.

## ðŸš€ Features

- **AI-Powered Test Generation**: Convert user stories to Gherkin features and Playwright specs using Claude AI
- **Multi-Layer Testing**: UI, API, visual regression, and performance testing
- **Intelligent Reporting**: Automated diff analysis, summaries, and KPI tracking  
- **CI/CD Integration**: GitHub Actions workflow with manual test suite selection
- **Notification System**: Slack/Teams webhook integration
- **Performance Monitoring**: k6 load testing with automated reporting
- **Visual Regression**: Screenshot comparison with pixel-perfect diff detection

Built with **Playwright**, **TypeScript**, and **Node.js** - runs locally and in CI environments.

---

## ðŸ“‹ Prerequisites

### System Requirements
- **Node.js LTS** (>= 20.x) - [Download here](https://nodejs.org/)
- **Git** - For version control and CI/CD
- **Modern browser** - Chrome/Chromium (installed automatically by Playwright)

### Optional Tools
- **k6** - For performance testing ([Install guide](https://k6.io/docs/get-started/installation/))
- **Anthropic API Key** - For AI-powered test generation ([Get API key](https://console.anthropic.com/))

### Quick Setup

1. **Clone the repository**
   ```bash
   git clone <your-repo-url>
   cd qa-ai-starter
   ```

2. **Install dependencies**
   ```bash
   npm install
   npx playwright install --with-deps
   ```

3. **Configure environment**
   ```bash
   cp .env.example .env
   # Edit .env with your settings (see Environment section below)
   ```

4. **Verify installation**
   ```bash
   npm run demo
   ```

### Verification
After running `npm run demo`, you should see:
- âœ… UI tests executed successfully
- âœ… Visual regression baseline created
- âœ… Reports generated in `reports/` directory

---

## ðŸ—ï¸ Project Structure

```
qa-ai-starter/
â”œâ”€â”€ ðŸ“ .github/workflows/     # CI/CD automation
â”‚   â””â”€â”€ ci.yml               # GitHub Actions pipeline
â”œâ”€â”€ ðŸ“ ai/                   # AI integration
â”‚   â””â”€â”€ claudeAgent.ts       # Claude AI client
â”œâ”€â”€ ðŸ“ features/             # Gherkin feature files (generated)
â”‚   â”œâ”€â”€ US-1-login.feature
â”‚   â””â”€â”€ US-2-logout.feature
â”œâ”€â”€ ðŸ“ fixtures/             # Test setup and utilities
â”‚   â”œâ”€â”€ global-setup.ts      # Authentication setup
â”‚   â””â”€â”€ test-extend.ts       # Custom test fixtures
â”œâ”€â”€ ðŸ“ pages/                # Page Object Models
â”‚   â”œâ”€â”€ LoginPage.ts
â”‚   â””â”€â”€ DashboardPage.ts
â”œâ”€â”€ ðŸ“ reports/              # Test results and analysis
â”‚   â”œâ”€â”€ api/                 # API diff reports
â”‚   â”œâ”€â”€ performance/         # k6 performance results
â”‚   â”œâ”€â”€ summaries/           # AI-generated summaries
â”‚   â””â”€â”€ ui/                  # Visual regression reports
â”œâ”€â”€ ðŸ“ sample-data/          # Test data and baselines
â”‚   â””â”€â”€ api/                 # API response samples
â”œâ”€â”€ ðŸ“ scripts/              # Automation and utility scripts
â”‚   â”œâ”€â”€ features_to_specs_ai.ts    # AI-powered spec generation
â”‚   â”œâ”€â”€ story_to_feature.ts        # Story to Gherkin conversion
â”‚   â”œâ”€â”€ api_diff.ts                # API comparison tool
â”‚   â”œâ”€â”€ summarize.ts               # Report summarization
â”‚   â””â”€â”€ k6/load_test.js            # Performance testing
â”œâ”€â”€ ðŸ“ stories/              # User stories (YAML format)
â”‚   â”œâ”€â”€ US-1-login.yml
â”‚   â””â”€â”€ US-2-logout.yml
â”œâ”€â”€ ðŸ“ tests/                # Test specifications
â”‚   â”œâ”€â”€ api/                 # API tests
â”‚   â””â”€â”€ e2e/                 # End-to-end UI tests
â”‚       â”œâ”€â”€ regression/      # Regression test suites
â”‚       â”œâ”€â”€ smoke/           # Smoke test suites
â”‚       â””â”€â”€ visual/          # Visual regression tests
â”œâ”€â”€ ðŸ“ utils/                # Shared utilities
â”‚   â”œâ”€â”€ testData.ts          # Test data management
â”‚   â””â”€â”€ mockApi.ts           # API mocking utilities
â”œâ”€â”€ ðŸ“ visual-baseline/      # Screenshot baselines
â”œâ”€â”€ .env.example             # Environment template
â”œâ”€â”€ package.json             # Dependencies and scripts
â”œâ”€â”€ playwright.config.ts     # Playwright configuration
â””â”€â”€ tsconfig.json           # TypeScript configuration
```

### Key Directories

- **`stories/`** - YAML user stories that drive test generation
- **`features/`** - Auto-generated Gherkin feature files  
- **`tests/`** - Playwright test specifications (UI, API, visual)
- **`pages/`** - Page Object Models for maintainable UI tests
- **`scripts/`** - AI-powered automation and utility scripts
- **`reports/`** - Generated test reports, diffs, and summaries

---

## ðŸŽ¯ Getting Started Guide

### 1. Basic UI Testing
Start with simple Playwright UI tests to verify your setup:
```bash
# Run smoke tests (fastest feedback)
npm run test:smoke

# Run all UI tests
npm run test:ui

# Run with debug mode
npm run test:ui:debug
```

### 2. AI-Powered Test Generation
Convert user stories into automated tests using AI:
```bash
# Convert YAML stories to Gherkin features
npm run ai:stories

# Generate Playwright specs from features using AI
npm run ai:specs:all

# Run the complete AI pipeline (stories â†’ features â†’ specs)
npm run pipeline:ai:all
```

### 3. Visual Regression Testing
Capture and compare screenshots for visual changes:
```bash
# Run visual regression tests
npm run test:visual

# Generate visual diff reports
npm run regression:ui
```

### 4. API Testing & Monitoring
Test and monitor API endpoints:
```bash
# Run API test suite
npm run test:api

# Compare API responses and generate diff reports
npm run diff:api
```

### 5. Performance Testing
Monitor application performance with k6:
```bash
# Run performance tests locally
npm run perf

# View performance reports in reports/performance/
```

### 6. Reporting & Analytics
Generate comprehensive test reports:
```bash
# Create summary reports (with AI insights if configured)
npm run report:summary

# Generate KPI and ROI reports
npm run report:kpi

# Send notifications to Slack/Teams
npm run notify "Test run completed"
```

### 7. Complete Demo Pipeline
Run the full testing pipeline:
```bash
# Execute complete demo (Generated spec + Smoke + UI + regression + reports + AI summary)
npm run demo
```

---

##  Environment (.env)

- `BASE_URL` â€“ site under test (default uses https://example.com)
- `OPENAI_API_KEY` â€“ optional, enables AI generation & summaries
- `OPENAI_MODEL` â€“ optional, defaults to `gpt-4o-mini`
- `SLACK_WEBHOOK_URL` â€“ optional (for Slack)
- `TEAMS_WEBHOOK_URL` â€“ optional (for Microsoft Teams)
- `BASELINE_TEST_CASE_AUTHORING_MIN` â€“ baseline minutes/test case (default 30)
- `AI_AUTOMATION_RATE` â€“ percent (0â€“100) of test case authoring automated by AI (default 30)
- `K6_VUS`, `K6_DURATION` â€“ performance test params

> If you donâ€™t set `OPENAI_API_KEY`, the scripts still work with **rule-based** fallback logic.

---

## ðŸš€ CI/CD Integration (GitHub Actions)

The project includes a comprehensive GitHub Actions workflow that automatically:

### Automated Pipeline Features
- **AI Test Generation**: Converts stories to features and specs using Claude AI
- **Multi-Suite Testing**: Runs UI, API, visual regression, and performance tests
- **Smart Execution**: PRs run smoke tests, pushes run full test suite
- **Manual Control**: Workflow dispatch allows selecting specific test suites
- **Performance Monitoring**: k6 load testing on non-PR events
- **Intelligent Reporting**: Generates diffs, summaries, and KPI reports

### Workflow Configuration
```yaml
# Triggers
- Pull requests (smoke tests)
- Push to main/master (full suite)
- Scheduled runs (nightly regression)
- Manual dispatch (custom test selection)
```

### Required GitHub Secrets
```bash
ANTHROPIC_API_KEY    # For AI test generation
USERNAME             # Test application username (optional)
PASSWORD             # Test application password (optional)
```

### Optional GitHub Variables
```bash
BASE_URL             # Override default test URL
```

### Manual Workflow Dispatch
You can manually trigger the workflow with specific test suites:
- **All**: Complete test suite
- **Smoke**: Quick validation tests
- **UI**: User interface tests only
- **API**: API endpoint tests only
- **Visual**: Visual regression tests only

### Artifacts & Reports
Each run generates:
- Test execution reports
- Visual regression diffs
- Performance test results
- AI-generated summaries
- KPI and ROI metrics

Artifacts are retained for 7 days and can be downloaded from the Actions tab.

---

## ðŸ†˜ Support & Resources

### Getting Help
- **Code Documentation**: All scripts include detailed comments and examples
- **Test Examples**: Check `tests/` directory for implementation patterns
- **Page Objects**: Review `pages/` for maintainable UI test patterns
- **AI Scripts**: Examine `scripts/` for automation and AI integration examples

### Common Issues & Solutions

#### AI Generation Not Working
- Verify `ANTHROPIC_API_KEY` is set correctly
- Check API key permissions and quota
- Review error logs in CI/CD runs

#### Tests Failing in CI
- Ensure all required secrets are configured
- Check browser compatibility settings
- Verify base URL accessibility

#### Visual Regression Issues
- Update baselines after intentional UI changes
- Check screenshot resolution settings
- Review visual diff threshold configuration

#### Performance Test Problems
- Verify k6 installation for local runs
- Check target application load capacity
- Adjust VU count and duration settings

### Best Practices
- **Start Small**: Begin with smoke tests before full regression
- **Use Page Objects**: Maintain tests with the Page Object Model pattern
- **AI Iteration**: Refine AI prompts based on generated test quality
- **Regular Updates**: Keep dependencies and baselines current
- **Monitor Reports**: Review KPI trends for continuous improvement

### Contributing
- Follow TypeScript best practices
- Add tests for new functionality
- Update documentation for changes
- Use conventional commit messages

### Troubleshooting
For issues not covered here:
1. Check the `reports/` directory for detailed logs
2. Review GitHub Actions run details
3. Examine individual script outputs
4. Verify environment configuration
